---
title: "Analyzing Economic Shocks"
author: "Ziling Zhang, Leah Hunt, and Jiahui Cheng"
subtitle: "Stat 470W"
date: "11/17/2020"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
---
\newpage
```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = FALSE,message=FALSE,warning = FALSE)
library(lubridate)
library(ggplot2)
library("tidyverse")
library("cluster")
library("factoextra")
library(tidyr)
data <- read.table("economicData.txt", header = TRUE)
data$inflation <- c(NA, (data$cpi[2:length(data$cpi)] - data$cpi[1:length(data$cpi)-1])/data$cpi[1:length(data$cpi)-1])
data <- data[,c("DATE", "DOW_Close", "unemployment", "oil", "gold", "federal_funds_rate", "consumer_confidence", "inflation")]
data <- na.omit(data)
datad <- read.table("economicDataDaily.txt", header = TRUE)
```

# Abstract
An economic shock is an event that causes a, generally sudden, change in the economy. Examples of shocks include the introduction of COVID-19, the deadly disease that has been spreading globally throughout 2020. This analysis seeks to examine the structure of the economy in such shock periods in comparison to non-shock periods. We will first identify shock periods using data then apply regression and time series modelling to analyze the differences in economic structure between periods identified as shocks and those not identified.


# Introduction

## Background and Motivation

The COVID-19 pandemic, which originated in Wuhan, China at the end of 2019 and has since spread globally, has largely impacted society on both social and economic levels. An analysis by Zekra identifies the largest economic impacts of the virus to be medical treatment costs, extensive use of limited medical resources that takes away from other health issues, economic disruption from social distancing and quarantining, decreases in tourism, and impacts on foreign investment. (Zekra) In this sense, the COVID-19 pandemic is but one example of an economic shock, which we will consider a period of rapid change incited by a particular event, which often causes rapid change in the economy.

When considering these shock periods, clearly there will be differences in some aspects of their impacts. For example, the shock caused by the 9/11 terrorist attacks would not feature major effects to the healthcare system nationally or restrictions imposed by social distancing. As Zeshan explores through simulation in relation to the COVID-19 shock, we can even see different industries be hit in different ways and with different magnitudes in particular shocks, thus adding a further level of complexity. (Zeshan) We would, however, still expect some similarities. For instance, in our prior example, both the pandemic and the 9/11 terrorist attacks would feature a decrease general outlook, which we will quantify through consumer confidence, and impacted the airline industry. 

For this analysis, we will take a larger scale approach to exploring the effects of shocks on the economy. We will attempt to model economic structure as a whole using common metrics of economic performance as opposed to considering particular impacted industries like the approach from Zeshan. In particular, we will use regression and time series techniques to develop models representing the economy's structure in order to identify and analyze shock periods, in particular comparing relative to the effects of the COVID-19 shock.

## Assumptions on Economic Structure

In order to complete our analysis, we will assume the following structure for the economy as a whole...

(THIS STILL NEEDS TO BE FINISHED, BUT I AM THINKING THIS WOULD BE A LOGICAL PLACE FOR THE STRUCTURE ASSUMPTIONS, I.E. TREND = LINEAR GENERAL + PERIODIC TS NATURAL FLUCTUATION + SHOCKS + RANDOM ERROR/NOISE)


## Data Description

For this analysis, we will be using data collected primarily through Federal Reserve Economic Data (FRED) and yahoo finance which will cover a range of common economic indicators and stock prices. The first set of variables creates our monthly dataset, i.e. data that is reported on a monthly basis, and consists of the unemployment rate, consumer confidence index, DOW Jones Composite (taken on the first of the month), oil price, federal funds rate (taken on the first of the month), and the inflation rate.
We also consider a daily data set, i.e. data reported for each day, which consists of the DOW Jones Composite, gold price, S&P 500, New York Stock Exchange, and 10 year bond rates. A more detailed explanation of these variables is in Appendix A.

## Research Questions
THIS ENTIRE SECTION NEEDS REVISED!!!!

**Question 1:** How do shocks like COVID-19 effect the economy?

**Question 2:** What are the differences and similarities for different shocks, in particular comparing COVID-19 to other shocks such as the Great Recession?

## Statistical Research Questions

**Question 1:** How does the structure of the economy, as defined by the effectiveness of models used to predict the DOW, differ for shock periods. In particular, do shock periods have higher magnitude residuals than non-shock periods? 

**Question 2:** Are there significant differences between shocks in their ability to be fit to a model of economic structure and in the magnitude and nature of their effects? 



# Exploratory Data Analysis

The main variable that we will consider as a response is the DOW, and as such, we want to get a better understanding of the DOW data. Looking at the raw DOW data, shown in the leftmost plot of Figure 1, we see that the shape is not linear, rather resembling a polynomial or exponential shape. In order to apply our theory of a linear general trend to the data, we will want to apply a transformation to the data; the log transform shown in the center plot of Figure 1 creates the more linear trend that we are aiming for. 

```{r eda1, fig.cap="The above figure shows different methods of showing the DOW. Notice that the center plot is more linear than the leftmost, which justifies the log transform. For analysis, we will also consider the changes in DOW, represented in the rightmost plot.", fig.height=3}
par(mfrow =c(1,3))
plot(ymd(data$DATE), data$DOW_Close, ylab = "DOW", xlab = "Date", main = "DOW over Time")
plot(ymd(data$DATE), log(data$DOW_Close), ylab = "log DOW", xlab = "Date", main = "Logarithm of \nDOW over Time")
DowDiff <- log(data$DOW_Close)[2:length(data$DOW_Close)] - log(data$DOW_Close)[1:length(data$DOW_Close)-1]
plot(ymd(data$DATE)[2:length(data$DOW_Close)], DowDiff, ylab = "log DOW Differences", xlab = "Date", main = "Logarithm of DOW \nDifferences over Time")
```

For our analysis, we will also want to consider the changes in the DOW, primarily for the sake of meeting the stationarity assumption. We see in the rightmost plot of Figure 1 that these values appear randomly around 0, meaning that we may not expect to find a significant trend over time. This is consistent with economic theory, which claims that we will not be able to predict these changes well. Keep in mind that even as we attempt to do so throughout this work, we will only be aiming to model the general structure with no predictive power. We do, however, see several points with larger changes, which we will analyze as we attempt to identify outliers.


Figure 2 considers similar ideas for the daily DOW data. While the first two plots look very similar, the third really emphasizes the main difference between using monthly and daily data. The daily data has much more noise, which is partially a product of having much more data to work with. In the rightmost plot, we again see a lot of seemingly random variation around 0 but with a some periods with seemingly higher variation. 

```{r eda2, fig.cap="The above figure shows different methods of showing the DOW using the daily data. Notice that the center plot is more linear than the leftmost, which justifies the log transform. For analysis, we will also consider the changes in DOW, represented in the rightmost plot.", fig.height=3}
par(mfrow =c(1,3))
plot(ymd(datad$DATE), datad$DOW, ylab = "DOW", xlab = "Date", main = "DOW over Time")
plot(ymd(datad$DATE), log(datad$DOW), ylab = "log DOW", xlab = "Date", main = "Logarithm of \nDOW over Time")
DowDiffd <- log(datad$DOW)[2:length(datad$DOW)] - log(datad$DOW)[1:length(datad$DOW)-1]
plot(ymd(datad$DATE)[2:length(datad$DOW)], DowDiffd, ylab = "log DOW diff", xlab = "Date", main = "Logarithm of DOW \nDifferences over Time")
```


Of course, we also want to consider the explanatory variables and their relationship to the DOW. Further plots and analysis considering these variables can be found in Appendix B.


# Identifying and Defining Shock Periods

We define shock periods as periods of large change, excluding change from the general trend, in the DOW Jones composite score, using this metric as our indicator for the economy as a whole. We will consider two potential periods for shock identification: by month and by day. Each of these analyses will also use the logarithm of the DOW instead of the DOW itself in order to have a general trend closer to linear, as discussed in the exploratory analysis. For the remainder of this section, any mention of the DOW score can be assumed to be referring to the logarithm of the DOW unless otherwise stated.

## Shock Identification by Month

Our first step in identifying shock periods is to exclude the natural trend using linear regression to model the DOW with time as the only predictor. The results of this step are shown in the below figure. 

```{r regMonthly, fig.cap = "This figure shows the results of the linear fit to remove the general trend. While this fit would generally not be considered a good fit to the data, its purpose is only to remove a generic trend, which it effectively does.", fig.height=3}
reg <- lm(log(DOW_Close)~
            poly(time(DATE),degree = 1,raw = T),data=data)
# NONE OF THESE PLOTS ARE PROPERLY LABELLED YET AND THIS IS NOT KNITTING RIGHT!!!
par(mfrow =c(1,2))
plot(log(data$DOW_Close))
abline(reg, col = "red")
plot(ymd(data$DATE), reg$residuals,ylab = "residuals", xlab = "Date")
```

Note that in the process of using a linear model and even fitting our line through linear regression, we are not intending to model our data in this step, but rather model, and then remove, only one small portion of the trend in the data. 

Now that we have removed the general trend, we must consider the changes in the DOW, i.e. consider the changes between the residuals of our linear model. We will use a time series model in order to understand the trend in these changes. We will consider outliers, defined as having a residual of more than three standard deviations, where the standard deviation is calculated from the residuals themselves, from 0. A visualization of the residuals of the time series model and the cutoffs are shown below. 

```{r, fig.cap = "This figure shows the model residuals. The red lines represent three standard deviations from 0. The seven points below the lower red line are the seven identified shocks.", fig.height=3}
# Difference between residuals
res_temp <- reg$residuals[2:length(reg$residuals)] - reg$residuals[1:length(reg$residuals)-1]
mod1 <- arima(res_temp,order = c(1,0,0))
outliersmonsig <- which(abs(mod1$residuals) > 3*sqrt(var(mod1$residuals)))
#outliers <- forecast::tsoutliers(mod1$residuals)$index
#data$DATE[outliers+1]
#data$DATE[which(abs(mod1$residuals)>3*sqrt(var(mod1$residuals)))]
plot(ymd(data$DATE)[2:length(reg$residuals)], res_temp, ylab = "Change in residual", xlab = "Date")
abline(h = -3*sqrt(var(mod1$residuals)), col = "red") # 3 standard deviations below
abline(h = 3*sqrt(var(mod1$residuals)), col = "red") # 3 standard deviations above
```

We see that the analysis identifies seven points as shocks. In order to verify these points, we can trace them back to actual events. The dates of these seven shocks and their associated events are shown below. 

```{r}
A <- c("October 1987", "Black Monday stock market crash on October 19, 1987")
B <- c("August 1990", "Beginning of the Gulf War (officially started August 2, 1990)")
C <- c("August 1998", "Russian financial crisis")
D <- c("September 2001", "9/11 terrorist attack")
E <- c("October 2008", "Great Recession; TARP relief bill signed")
G <- c("February 2009" , "Great Recession; congress passes large stimulus package")
H <- c("March 2020", "First major reactions to COVID-19 pandemic begin in the US")

varExplanations <- matrix(c(A, B,C,D,E,G,H), ncol = 2, byrow = TRUE)
varExplanations <- as.data.frame(varExplanations)
#varExplanations
colnames(varExplanations) <- c("Time", "Event")
knitr::kable(varExplanations, 
             caption = "Associated events for each identified shock period",
             align = c('c', rep('l', 2))) %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "condensed"),
                          font_size = 9, latex_options = "HOLD_position")
```

## Shock Identification by Day

In order to consider the shocks by daily data, we will use the same process as we used for the monthly data on its daily counterpart. The major difference will be the magnitude of data and the level of volatility in the data as daily data features much more noise than the monthly data. 

Once again, we start by removing the general linear trend, as seen in the below figure.

```{r regdaily, fig.cap = "This figure shows the results of the linear fit to remove the general trend. While this fit would generally not be considered a good fit to the data, its purpose is only to remove a generic trend, which it effectively does.", fig.height=3}
regd <- lm(log(DOW)~
            poly(time(DATE),degree = 1,raw = T),data=datad)
# NONE OF THESE PLOTS ARE PROPERLY LABELLED YET AND THIS IS NOT KNITTING RIGHT!!!
par(mfrow =c(1,2))
plot(log(datad$DOW))
abline(regd, col = "red")
plot(ymd(datad$DATE), regd$residuals,ylab = "residuals", xlab = "Date")
```

Then, we consider the changes in these residuals between days and run a time series model on these changes, again looking for residuals that are further than three standard deviations from 0. 

```{r, fig.cap = "This figure shows the model residuals. The red lines represent three standard deviations from 0. The shocks are the points that do not fall between the red lines. The blue lines show the periods that were considered shocks in the monthly analysis for reference.", fig.height=3}
# Difference between residuals
res_tempd <- regd$residuals[2:length(regd$residuals)] - regd$residuals[1:length(regd$residuals)-1]
mod1d <- arima(res_tempd,order = c(1,0,0))
#outliers <- forecast::tsoutliers(mod1$residuals)$index
#data$DATE[outliers+1]
#data$DATE[which(abs(mod1$residuals)>3*sqrt(var(mod1$residuals)))]
outliersmonsigd <- which(abs(mod1d$residuals) > 3*sqrt(var(mod1d$residuals)))
plot(ymd(datad$DATE)[2:length(regd$residuals)], res_tempd, ylab = "Change in residual", xlab = "Date")
abline(h = -3*sqrt(var(mod1d$residuals)), col = "red") # 3 standard deviations below
abline(h = 3*sqrt(var(mod1d$residuals)), col = "red") # 3 standard deviations above
abline(v = ymd(data$DATE[outliersmonsig]), col = "blue") #IDK if we should keep these
abline(v = ymd(data$DATE[outliersmonsig+1]), col = "blue") #Also, check if this is doing what it is supposed to...
```

Compared to the monthly case, we see many more shock points. We see, however, that the points that we had previously identified as shocks do appear to some degree using this method as well with all seven cases having at least one shock day within the month and some cases like Black Monday showing up as distinct days.  

# Modelling

## Analysis on Monthly Data

## Analysis on Daily Data


# Discussion

# Limitations and Future Study


# Works Cited

# Appendix A: Full Variable Description
```{r}
date <- c("DATE", "", "Date on which observations were taken; reported as the first of the month")
dow <- c("DOW", "Dollars", "Gives the value of the DOW Jones Composite as of closing on the first of the month")
unemployment <- c("unemployment", "", "US unemployment rate (unemployed/total people)")
oil <- c("oil", "dollars/barrel", "Price of oil; based out of West Texas")
gold <- c("gold", "dollers/ounce", "Price of gold as of the first of the month; based on London Bullion Market")
cpi <- c("cpi", "relative to 1982-1984" , "Consumer price index; amount that 1 dollar is worth in 1982-1984 dollars")
ffr <- c("ffr", "", "Federal Funds Rate  set by the Federal Reserve")
cc <- c("consumer_confidence", "", "Consumer confidence using the University of Michigan Consumer Sentiment index with 1966 as the base year")

varExplanations <- matrix(c(date, dow, unemployment, oil, gold, cpi, ffr, cc), ncol = 3, byrow = TRUE)
varExplanations <- as.data.frame(varExplanations)
#varExplanations
colnames(varExplanations) <- c("Variable Name", "Unit", "Description")
knitr::kable(varExplanations, 
             caption = "Full description of all monthly variables",
             align = c('c', rep('l', 2))) %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "condensed"),
                          font_size = 9, latex_options = "HOLD_position")
```

```{r}
date <- c("DATE", "", "Date on which observations were taken; reported as the first of the month")
dow <- c("DOW", "dollars", "Value of the DOW Jones Composite as of closing on the particular day")
nyse <- c("nyse", "dollars", "Value for the New York Stock Exchange on the particular day as of closing")
bond <- c("bond10", "dollars", "10 year US bond prices")
sp <- c("sp", "dollars", "Value of the S&P 500 Index")
gold <- c("gold", "dollers/ounce", "Price of gold; based on London Bullion Market")

varExplanations <- matrix(c(date, dow, nyse, bond, sp, gold), ncol = 3, byrow = TRUE)
varExplanations <- as.data.frame(varExplanations)
#varExplanations
colnames(varExplanations) <- c("Variable Name", "Unit", "Description")
knitr::kable(varExplanations, 
             caption = "Full description of all daily variables",
             align = c('c', rep('l', 2))) %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "condensed"),
                          font_size = 9, latex_options = "HOLD_position")
```

May want to write in more on how the DOW, S&P, nyse, etc. are calculated and what each of these are relevant for


# Appendix B: Additional Plots and Exploratory Analysis

```{r}
par(mfrow =c(2,3))
for(i in c(2:6)){
  plot(ymd(data$DATE), data[,i], ylab = colnames(data)[i])
  abline(v = ymd(data$DATE[outliersmonsig]), col = "blue") #IDK if we should keep these
abline(v = ymd(data$DATE[outliersmonsig+1]), col = "blue") #Also, check if this is
}

```

```{r}
par(mfrow =c(2,2))
for(i in c(3:6)){
  plot <- ggplot(aes(x = data[,i], y = data$DOW_Close, color = ymd(data$DATE)), data = data) +
    geom_point() +
    xlab(colnames(data)[i])
  print(plot)
}
```

```{r}
dataDiffs <- data
for(col in 2:ncol(data)){
  for(row in 2:nrow(data)){
    dataDiffs[row, col] <- log(data[row, col]) - log(data[row-1, col])
  }
}
par(mfrow =c(2,3))
for(i in c(2:6)){
  plot(ymd(dataDiffs$DATE), dataDiffs[,i], ylab = colnames(dataDiffs)[i], ylim = c(-1,1))
  abline(v = ymd(dataDiffs$DATE[outliersmonsig]), col = "blue") #IDK if we should keep these
abline(v = ymd(dataDiffs$DATE[outliersmonsig+1]), col = "blue") #Also, check if this is
}

```
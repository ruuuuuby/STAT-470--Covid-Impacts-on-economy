---
title: "Analyzing Economic Shocks"
author: "Ziling Zhang, Leah Hunt, and Jiahui Cheng"
subtitle: "Stat 470W"
date: "11/17/2020"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
header-includes: 
- \usepackage{float}    
---
\newpage
```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = FALSE,message=FALSE,warning = FALSE, fig.pos = "H", out.extra = "")
library(lubridate)
library(ggplot2)
library("tidyverse")
library("cluster")
library("factoextra")
library(stats)
library(tidyr)
set.seed(470)
data <- read.table("economicData.txt", header = TRUE)
data$inflation <- c(NA, (data$cpi[2:length(data$cpi)] - data$cpi[1:length(data$cpi)-1])/
                  data$cpi[1:length(data$cpi)-1])
data <- data[,c("DATE", "DOW_Close", "unemployment", "oil", "gold",
                "federal_funds_rate", "consumer_confidence", "inflation")]
data <- na.omit(data)
datad <- read.table("economicDataDaily.txt", header = TRUE)
```

# Abstract
An economic shock is an event that causes a, generally sudden, change in the economy. Examples of shocks include the introduction of COVID-19, the deadly disease that has been spreading globally throughout 2020. This analysis seeks to examine the structure of the economy in such shock periods in comparison to non-shock periods. We will first identify shock periods using data then apply regression and time series modelling to analyze the differences in economic structure between periods identified as shocks and those not identified.


# Introduction

## Background and Motivation

The COVID-19 pandemic, which originated in Wuhan, China at the end of 2019 and has since spread globally, has largely impacted society on both social and economic levels. An analysis by Zekra identifies the largest economic impacts of the virus to be medical treatment costs, extensive use of limited medical resources that takes away from other health issues, economic disruption from social distancing and quarantining, decreases in tourism, and impacts on foreign investment. (Zekra) In this sense, the COVID-19 pandemic is but one example of an economic shock, which we will consider a period of rapid change incited by a particular event, which often causes rapid change in the economy.

When considering these shock periods, clearly there will be differences in some aspects of their impacts. For example, the shock caused by the 9/11 terrorist attacks would not feature major effects to the healthcare system nationally or restrictions imposed by social distancing. As Zeshan explores through simulation in relation to the COVID-19 shock, we can even see different industries be hit in different ways and with different magnitudes in particular shocks, thus adding a further level of complexity. (Zeshan) We would, however, still expect some similarities. For instance, in our prior example, both the pandemic and the 9/11 terrorist attacks would feature a decrease general outlook, which we will quantify through consumer confidence, and impacted the airline industry. 

For this analysis, we will take a larger scale approach to exploring the effects of shocks on the economy. We will attempt to model economic structure as a whole using common metrics of economic performance as opposed to considering particular impacted industries like the approach from Zeshan. In particular, we will use regression and time series techniques to develop models representing the economy's structure in order to identify and analyze shock periods, in particular comparing relative to the effects of the COVID-19 shock.

## Assumptions on Economic Structure

In order to complete our analysis, we will be making a few assumptions on the economic structure as a whole. First, we will consider the DOW Jones Composite Average to be a reasonable metric for overall economic performance and thus will use this as our response variable in modelling. We consider the DOW Jones Composite Average a reasonable indicator of economic performance because it includes data from 65 major companies including ones from each of the major DOW sectors: industrial, utility, and transportation, thus giving a somewhat broad look at the economy overall. 

We will also assume that economic trends as a whole, in our case being modelled by the DOW, can be split into three types: a general trend that can be modelled as a linear function, periodic fluctuations that can be modelled through time series, and shocks, which will show up as larger amounts of variation remaining after accounting for the first two, noting that we will still have to distinguish between random error and shocks at this stage. We will assume that shocks have a large enough impact to be distinguished from the random error in the data. An illustration of this structure is shown in the below figure.

```{r tot, fig.cap="This figure illustrates the economic trends over time as described by our model. The black line represents the general trend with the red curve showing the natural fluctuations added to this general trend. The blue segments show two examples of shocks. Note that the shock around 1.4 is a negative shock and the one around 3.4 is a positive shock. The purple curve shows the sum of all of these components plus a small amount of noise, i.e. random error. Note that the units of the axis in this plot are arbitrary and that this plot was artificially generated with the sole intention of explaining the model.", fig.height=3}
set.seed(470)
curve(x + jitter(.5*sin(4*x), amount = .1) + (x < 3.45 & x > 3.3)*.5 + (x < 1.45 & x > 1.3)*-.5, 
      col = "purple", xlim = c(0,4),xlab = "Time", ylab = "Economic Trend", 
      main = "Economic Trends Over Time")
curve(x + .5*sin(4*x) + (x < 3.45 & x > 3.3)*.5 + (x < 1.45 & x > 1.3)*-.5, col = "blue", add = TRUE)
curve(1*x, add = TRUE)
curve(x + .5*sin(4*x), add = TRUE, col = "red")
```

More formally, we can describe this structure as treating the economic structure as a three level model.


The first level is the general trend, which is represented by a linear model on the DOW:
\[
Y_{ijk} = \beta_0 + \beta_1t + a_{ij} + \epsilon_{ijk} 
\]
where $\epsilon_{ijk}$ ~ $N(0, \sigma^2)$.


The second level handles the natural fluctuations using a time series model:

\[
a_{ij} = \theta_1a_{ij-1} + u_{ij} + w_i
\]

where $u_{ij}$ ~ $N(0, \sigma_u^2)$.


The third level then represents the shocks, which for the sake of the model will be assumed to all have the same magnitude, even though this is a rather strong simplification of reality that will be accounted for in the error terms in our model:

\[
w_i = w_0 + \beta_2v_i + k_i
\]

where $k_{i}$ ~ $N(0, \sigma_k^2)$ and $v_i$ is an indicator variable representing whether a period is a shock period.

## Data Description

For this analysis, we will be using data collected primarily through Federal Reserve Economic Data (FRED) and yahoo finance which will cover a range of common economic indicators and stock prices. The first set of variables creates our monthly dataset, i.e. data that is reported on a monthly basis, and consists of the unemployment rate, consumer confidence index, DOW Jones Composite (taken on the first of the month), oil price, federal funds rate (taken on the first of the month), and the inflation rate. Note that the inflation rate was originally reported as CPI (consumer price index); inflation was calculated as the percent change of CPI, so it is reported as monthly inflation.
We also consider a daily data set, i.e. data reported for each day, which consists of the DOW Jones Composite, gold price, S&P 500, New York Stock Exchange, and 10 year bond rates. A more detailed explanation of these variables is in Appendix A. As we proceed with analysis, we will also consider a log transform and a log difference transform on many of these variables.

## Research Questions

**Question 1:** How do shock periods and non-shock periods differ in economic structure?

**Question 2:** What are the differences and similarities for different shocks, in particular comparing COVID-19 to other shocks such as the Great Recession?

## Statistical Research Questions

**Question 1:** How does the structure of the economy, as defined by the effectiveness of models used to predict the DOW, differ for shock periods? In particular, do shock periods have higher magnitude residuals than non-shock periods? 

**Question 2:** Are there significant differences between shocks in their ability to be fit to a model of economic structure and in the magnitude and nature of their effects, as evidenced by their residuals in the fitted economic models? 



# Exploratory Data Analysis

The main variable that we will consider as a response is the DOW, and as such, we want to get a better understanding of the DOW data. Looking at the raw DOW data, shown in the leftmost plot of Figure 1, we see that the shape is not linear, rather resembling a polynomial or exponential shape. In order to apply our theory of a linear general trend to the data, we will want to apply a transformation to the data; the log transform shown in the center plot of Figure 1 creates the more linear trend that we are aiming for. Note that while the trend may appear mildly curved, much of this slight curved appearance comes from the rapid growth in the 1990's that was caused by the introduction of the Internet. Thus, this trend still reasonably falls under the assumptions of the model, though it does indicate that the introduction of the Internet may constitute an unindentified shock in our model.

```{r eda1, fig.cap="The above figure shows different methods of showing the DOW using the monthly data. Notice that the center plot is more linear than the leftmost, which justifies the log transform. For analysis, we will also consider the changes in DOW, represented in the rightmost plot.", fig.height=3}
par(mfrow =c(1,3))
plot(ymd(data$DATE), data$DOW_Close, ylab = "DOW", 
     xlab = "Date", main = "DOW over Time")
plot(ymd(data$DATE), log(data$DOW_Close), ylab = "log DOW", 
     xlab = "Date", main = "Logarithm of \nDOW over Time")
DowDiff <- log(data$DOW_Close)[2:length(data$DOW_Close)] -
  log(data$DOW_Close)[1:length(data$DOW_Close)-1]
plot(ymd(data$DATE)[2:length(data$DOW_Close)], DowDiff, ylab = "log DOW Differences",
     xlab = "Date", 
     main = "Logarithm of DOW \nDifferences over Time")
```

For our analysis, we will also want to consider the changes in the DOW, primarily for the sake of meeting the stationarity assumption. We see in the rightmost plot of Figure 1 that these values appear randomly around 0, meaning that we may not expect to find a significant trend over time. This is consistent with economic theory, which claims that we will not be able to predict these changes well. Keep in mind that even as we attempt to do so throughout this work, we will only be aiming to model the general structure with no predictive power. We do, however, see several points with larger changes, which we will analyze as we attempt to identify outliers.


Figure 2 considers similar ideas for the daily DOW data. While the first two plots look very similar, the third really emphasizes the main difference between using monthly and daily data. The daily data has much more noise, which is partially a product of having much more data to work with. In the rightmost plot, we again see a lot of seemingly random variation around 0 but with a some periods with seemingly higher variation. 

```{r eda2, fig.cap="The above figure shows different methods of showing the DOW using the daily data. Notice that the center plot is more linear than the leftmost, which justifies the log transform. For analysis, we will also consider the changes in DOW, represented in the rightmost plot.", fig.height=3}
par(mfrow =c(1,3))
plot(ymd(datad$DATE), datad$DOW, ylab = "DOW", xlab = "Date", main = "DOW over Time")
plot(ymd(datad$DATE), log(datad$DOW), ylab = "log DOW", xlab = "Date", 
     main = "Logarithm of \nDOW over Time")
DowDiffd <- log(datad$DOW)[2:length(datad$DOW)] - log(datad$DOW)[1:length(datad$DOW)-1]
plot(ymd(datad$DATE)[2:length(datad$DOW)], DowDiffd, ylab = "log DOW diff", 
     xlab = "Date", main = "Logarithm of DOW \nDifferences over Time")
```


Of course, we also want to consider the explanatory variables and their relationship to the DOW. Further plots and analysis considering these variables can be found in Appendix B.

At this stage, we also acknowledge that while we considered numerical summaries as an approach to exploring the data, we found that due to the temporal nature of the data, the data is much better understood graphically. Summary statistics like the sample arithmetic mean or five number summaries of the variables would hold very little meaning as these variables are not stable over time.


# Identifying and Defining Shock Periods

We define shock periods as periods of large change, excluding change from the general trend, in the DOW Jones composite score, using this metric as our indicator for the economy as a whole. We will consider two potential periods for shock identification: by month and by day. Each of these analyses will also use the logarithm of the DOW instead of the DOW itself in order to have a general trend closer to linear, as discussed in the exploratory analysis. For the remainder of this section, any mention of the DOW score can be assumed to be referring to the logarithm of the DOW unless otherwise stated.

## Shock Identification by Month

Our first step in identifying shock periods is to exclude the natural trend using linear regression to model the DOW with time as the only predictor. The results of this step are shown in the below figure. 

```{r regMonthly, fig.cap = "This figure shows the results of the linear fit to remove the general trend. While this fit would generally not be considered a good fit to the data, its purpose is only to remove a generic trend, which it effectively does.", fig.height=3}
reg <- lm(log(DOW_Close)~
            poly(time(DATE),degree = 1,raw = T),data=data)
par(mfrow =c(1,2))
plot(ymd(data$DATE), log(data$DOW_Close), xlab = "Date", ylab = "log DOW", 
     main = "Linear Trend of\n log DOW over Time")
curve(6.131019  + 0.006492 * 12 * (year(as.Date(x, origin = "1981-01-01"))-1992) + 
        0.006492 *month(as.Date(x, origin = "1981-01-01")-1), 
      col = "red", add = TRUE) # I have no idea why this line of code works...my guess is that you have to scale back by 12 to accommodate the multiplication but IDK why...
plot(ymd(data$DATE), reg$residuals,ylab = "residuals", xlab = "Date", 
     main = "Residuals from\nLinear Trend")
```

Note that in the process of using a linear model and even fitting our line through linear regression, we are not intending to model our data in this step, but rather model, and then remove, only one small portion of the trend in the data. 

Now that we have removed the general trend, we must consider the changes in the DOW, i.e. consider the changes between the residuals of our linear model. We will use a time series model in order to understand the trend in these changes. We will consider outliers, defined as having a residual of more than three standard deviations, where the standard deviation is calculated from the residuals themselves, from 0. A visualization of the residuals of the time series model and the cutoffs are shown below. 

```{r, fig.cap = "This figure shows the model residuals. The red lines represent three standard deviations from 0. The seven points below the lower red line are the seven identified shocks.", fig.height=3}
# Difference between residuals
res_temp <- reg$residuals[2:length(reg$residuals)] - reg$residuals[1:length(reg$residuals)-1]
mod1 <- arima(res_temp,order = c(1,0,0))
outliersmonsig <- which(abs(mod1$residuals) > 3*sqrt(var(mod1$residuals)))
plot(ymd(data$DATE)[2:length(reg$residuals)], res_temp, ylab = "Residual", 
     xlab = "Date", main = "Residuals of Time Series Model")
abline(h = -3*sqrt(var(mod1$residuals)), col = "red") # 3 standard deviations below
abline(h = 3*sqrt(var(mod1$residuals)), col = "red") # 3 standard deviations above
```

We see that the analysis identifies seven points as shocks. In order to verify these points, we can trace them back to actual events. The dates of these seven shocks and their associated events are shown below. 

```{r}
A <- c("October 1987", "Black Monday stock market crash on October 19, 1987")
B <- c("August 1990", "Beginning of the Gulf War (officially started August 2, 1990)")
C <- c("August 1998", "Russian financial crisis")
D <- c("September 2001", "9/11 terrorist attack")
E <- c("October 2008", "Great Recession; TARP relief bill signed")
G <- c("February 2009" , "Great Recession; congress passes large stimulus package")
H <- c("March 2020", "First major reactions to COVID-19 pandemic begin in the US")
varExplanations <- matrix(c(A, B,C,D,E,G,H), ncol = 2, byrow = TRUE)
varExplanations <- as.data.frame(varExplanations)
#varExplanations
colnames(varExplanations) <- c("Time", "Event")
knitr::kable(varExplanations, 
             caption = "Associated events for each identified shock period",
             align = c('c', rep('l', 2))) %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "condensed"),
                          font_size = 9, latex_options = "HOLD_position")
```

## Shock Identification by Day

In order to consider the shocks by daily data, we will use the same process as we used for the monthly data on its daily counterpart. The major difference will be the magnitude of data and the level of volatility in the data as daily data features much more noise than the monthly data. 

Once again, we start by removing the general linear trend, as seen in the below figure.

```{r regdaily, fig.cap = "This figure shows the results of the linear fit to remove the general trend. While this fit would generally not be considered a good fit to the data, its purpose is only to remove a generic trend, which it effectively does.", fig.height=3}
regd <- lm(log(DOW)~
            poly(time(DATE),degree = 1,raw = T),data=datad)
par(mfrow =c(1,2))
plot(ymd(datad$DATE), log(datad$DOW), xlab = "Date", ylab = "log DOW", 
     main = "Linear Trend of\n log DOW over Time")
abline(lm(log(datad$DOW)~ymd(datad$DATE)), col = "red")
plot(ymd(datad$DATE), regd$residuals,ylab = "Residuals", xlab = "Date", 
     main = "DOW over Time\n Residuals")
```

Then, we consider the changes in these residuals between days and run a time series model on these changes, again looking for residuals that are further than three standard deviations from 0. 

```{r, fig.cap = "This figure shows the model residuals. The red lines represent three standard deviations from 0. The shocks are the points that do not fall between the red lines. The blue lines show the periods that were considered shocks in the monthly analysis for reference.", fig.height=3}
# Difference between residuals
res_tempd <- regd$residuals[2:length(regd$residuals)] - regd$residuals[1:length(regd$residuals)-1]
mod1d <- arima(res_tempd,order = c(1,0,0))

outliersmonsigd <- which(abs(mod1d$residuals) > 3*sqrt(var(mod1d$residuals)))
plot(ymd(datad$DATE)[2:length(regd$residuals)], res_tempd, ylab = "Change in residual", 
     xlab = "Date", main = "Daily Residuals\nRelative to Shocks")
abline(h = -3*sqrt(var(mod1d$residuals)), col = "red") # 3 standard deviations below
abline(h = 3*sqrt(var(mod1d$residuals)), col = "red") # 3 standard deviations above
abline(v = ymd(data$DATE[outliersmonsig]), col = "blue")
abline(v = ymd(data$DATE[outliersmonsig+1]), col = "blue") 
```

Compared to the monthly case, we see many more shock points. We see, however, that the points that we had previously identified as shocks do appear to some degree using this method as well with all seven cases having at least one shock day within the month and some cases like Black Monday showing up as distinct days.  

# Modelling

## Approach

After identifying 7 shocks on both statistical and economic perspective, we want to examine these 7 shocks, comparing them with other regular periods by modelling the economic structure. As stated from the assumptions above, we assume economic structure is a linear combination of general trend, shocks, and fluctuations. Thus, our statistical approach to model economic structure is to use a linear model by considering time series effects, which are quantified by using cross correlation and then including correlated time periods. 

After modelling the economic structure, we tend to compare shocks and non-shock periods by looking at their residuals. 

## Analysis on Monthly Data

For the monthly data, we choose the response variable as *Dow_Close* value. We also choose 6 monthly reported economic predictors to measure the strucuture. Our goal is to examine how stock market (economy) will respond to changes in macrovarables. To normalize the data, we take log of the difference of all our variables except for inflation, which is equal to take the percent change of our values. More importantly, such transformation can satisfy our assumption of stationarity on response variable. 




Firstly, check the figure of *Dow_Close* after transformation. The figure will be similar to *eda2* and it is stationary, satisfying the assumptions of time series model. 
```{r}
# transform all variables
Dowm=as.ts(log(data$DOW_Close[2:474])-log(data$DOW_Close[1:473])) 
unemploymentm=na.omit(as.ts(log(data$unemployment[2:474]) - log(data$unemployment[1:473])))
oilm=na.omit(as.ts(log(data$oil[2:474]) - log(data$oil[1:473])))
goldm=na.omit(as.ts(log(data$gold[2:474]) - log(data$gold[1:473])))
ffrm=na.omit(as.ts(log(data$federal_funds_rate[2:474]) - log(data$federal_funds_rate[1:473])))
ccim=na.omit(as.ts(log(data$consumer_confidence[2:474]) - log(data$consumer_confidence[1:473])))
```

Now we will consider two modelling approaches. The first is a more naive model uses all predictors in a time series model. The second is more selective, considering only predictors that will be significant in the final model. The primary goal of using these models is to compare their results relative to the identified shock periods and to explore if the affects on the shock periods are consistent among them.

### Modelling Approach 1: All Predictor Time Series Model

For this approach, we will consider all of the predictors. Our first step will be to, as explained previously, remove the linear general trend of the data through regression, which has already been done as part of previous analysis. We will then consider a time series which predicts change in the DOW as a function of the log differences of all of the predictors, with the exception of inflation, which is already a percent change value and thus would be more reasonably be kept as is. 

```{r}
# EVERYTHING AS PERCENT CHANGE...well except for inflation because that already is a percent change

# Make time series data and only pick time where all variables have values
Dow=as.ts(log(data$DOW_Close[2:474])-log(data$DOW_Close[1:473])) 
unemployment=na.omit(as.ts(log(data$unemployment[2:474]) - log(data$unemployment[1:473])))
oil=na.omit(as.ts(log(data$oil[2:474]) - log(data$oil[1:473])))
gold=na.omit(as.ts(log(data$gold[2:474]) - log(data$gold[1:473])))
cpi=na.omit(as.ts(data$inflation[2:474]))
ffr=na.omit(as.ts(log(data$federal_funds_rate[2:474]) - log(data$federal_funds_rate[1:473])))
cci=na.omit(as.ts(log(data$consumer_confidence[2:474]) - log(data$consumer_confidence[1:473])))

# Mean structure modelling (General Trend)
regmodel=lm(Dow~unemployment+oil+gold+cpi+ffr+cci)

# AR(1)
arimamodel <- arima(residuals(regmodel),order = c(1,0,0))
```

```{r mod1acf, fig.cap="This plot show the ACF plot for the time series model applying to all of the monthly predictors. Notice that the only component with significant autocorrelation is at 0, which is implies that this data is meeting our assumptions."}
acf(resid(arimamodel))

```

```{r mod1residuals, fig.cap = "This figure shows the residuals of our first model, with the periods originally identified as shocks in red. Notice that the red points appear with a larger magnitude residual in this plot as they did in our shock identification plots."}
plot(ymd(data$DATE)[2:474][-outliersmonsig], resid(arimamodel)[-outliersmonsig], 
     ylab = "Residual", xlab = "Date", ylim = c(-.25, .2))
points(ymd(data$DATE)[outliersmonsig], resid(arimamodel)[outliersmonsig], col = "red")
```





### Modelling Approach 2: Subsetting Predictors 

For this approach, we will take the cross correlation between *Dow_Close* and 6 predictors. The goal of this step is to examine the effects of time series. All variables are not statistically independent since they all depend on time. One important feature for economic predictor is that current value might depend on previous values. Thus, before we examine the relationship among predictors in economic structure, we also want to consider the effects of the past. 

Using cross correlation figures, we can easily check past significant lags that will influence current value of predictors. So that, we can include those lags into consideration. 

From the cross correlation figures, we notice unemployment rate at lag -1 and -2, oil at lag -1, gold price at lag -1, inflation rate at -2,-3, federal funds rate at lag -2, and consumer confidence index at current lag and -1 are significant with current *Dow_Close* value. Thus, we need to consider them into the linear model. 

```{r, fig.cap = "This figure shows cross correlation between Dow and 6 predictors.The x-axis is the lag, while before 0 represents past and after future. Future analysis is meaningless here since we will not do prediction. Lag that is higher than blue line means it is significant", fig.height=3}

par(mfrow =c(2,3))
ccf(Dowm,unemploymentm)
ccf(Dowm,oilm)
ccf(Dowm,goldm)
ccf(Dowm,data$inflation)
ccf(Dowm,ffrm)
ccf(Dowm,ccim)

```


```{r, figure.cap='residuals of models, showing the real effects of shocks on percent change' ,fig.height=3}
# linear model 

alldata=ts.intersect(Dowm,un1 =stats::lag(unemploymentm,-1),un2=stats::lag(unemploymentm,-2),
                     oil1=stats::lag(oilm,-1), 
                     gold=stats::lag(goldm,-1), 
                     inf1=stats::lag(data$inflation,-2), inf2=stats::lag(data$inflation,-3),
                     ffr2=stats::lag(ffrm,-2),
                     cci=stats::lag(ccim,0),cci1=stats::lag(ccim,-1))

modelccf=lm(Dowm~un1+un2+oil1+gold+inf1+inf2+ffr2+cci+cci1,data=alldata)
summary(modelccf)

plot(ymd(data$DATE)[4:473],resid(modelccf),xlab="Date",ylab="percent change of Dow_Close")
points(ymd(data$DATE)[82],resid(modelccf)[78],col="red")
points(ymd(data$DATE)[116],resid(modelccf)[112],col="red")
points(ymd(data$DATE)[212],resid(modelccf)[208],col="red")
points(ymd(data$DATE)[249],resid(modelccf)[245],col="red")
points(ymd(data$DATE)[334],resid(modelccf)[330],col="red")
points(ymd(data$DATE)[338],resid(modelccf)[334],col="red")
points(ymd(data$DATE)[471],resid(modelccf)[467],col="red")

```

The resulting linear model shows that percent change of *Dow_Close* has positive relationship with oil price, gold price, current consumer confidence index, and past one period inflation rate. It also has negative relationship with past two periods federal funds rate. All make sense since increasing consumer confidence and inflation encourage investment on stock market and can boost economy. Decreasing federal funds rate means that the costs of borrowing decrease and can also boost economic transaction. 

One interesting fact shown by the model is that for all predictors that have two past periods in the model, such as unemployment at lag-1 and unemployment at lag -2, they have opposite relationship with *Dow_Close*. They also make sense since economy itself has the ability to self adjust. And we consider closer period has major effect on current economy. 

This model is our final model of economic structure, since when we analyze the residual, we notice that residuals follow independent, indentical structure, which satisfies the assumption of linear model. It also shows that all residuals left should be the effects of natural fluctuations and shocks. Thus, we can directly use residuals of this model to evaluate the differences of non-shock period and shock periods. 

As a result from Shock Identification part, we notice 7 shocks in total. Red points are effects of these 7 shocks and we can see that nearly all shocks are negative shocks, which means that they create negative percentage change of Dow in the past. 

Nearly all 7 shocks have above 10% decrease in *Dow_Close*. Otherwise, most non-shock periods remain in the fluctuations from 10% to -10%. Thus, we can assume that in a monthly period, a 10% fluctuation is normal in economy. While other factors intervene, a shock that create more than 10% fluctuation might happen. 

Moreover, comparing among these 7 shocks, we notice that nearly all of them have a 10% decrease except for the shock of black Monday stock market crash on October, 1987. It had over 20% decrease. However, the reason is that only this shock has a direct impact on stock market. 

The rightmost three red points are the covid shock and great recession. They caused similar effects and pattern and we can assume that covid has slightly worse impacts on economy than Great Recession. 



## Analysis on Daily Data

Daily information is also important since some shocks might only create impacts in only few days, but not months. Thus, we want to analyze the daily information during these shocks. 

We still choose the response variable as *Dow_Close* value, but we choose 4 new daily reported economic predictors to measure the structure. We will do similar approach on daily model and transform the data. Some data is missing since sometimes data such as during weekend will not being recorded. Thus, we replace missing data to previous day data.

```{r}
# Variable table INCOMPLETE
variable<-data.frame(
  No = c(1,2,3,4,5),
  Variable=c("Dow_Close","DGS10","SP500","gold","NYSE"),
  Description = 
    c(""), 
  Units = c(''))
knitr::kable(variable,
             caption = "Variable Attributes",
             align = c('c', rep('l', 7)),format="latex") %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "condensed"),
                          font_size = 7, latex_options = "HOLD_position")
```


```{r}
# Transformation

datad <- read.table("economicDataDaily.txt", header = TRUE)
datad$DGS10=as.numeric(datad$DGS10)
datad$gold=as.numeric(datad$gold)

a=which(is.na(datad$DGS10))
datad$DGS10[a]<-datad$DGS10[a-1]
b=which(is.na(datad$gold))
datad$gold[b]<-datad$gold[b-1]
c=which(is.na(datad$gold))
datad$gold[c]<-datad$gold[c-1]



Dowd=as.ts(log(datad$DOW[2:10011])-log(datad$DOW[1:10010])) 
Dgsd=as.ts(log(datad$DGS10[2:10011])-log(datad$DGS10[1:10010])) 
spd=as.ts(log(datad$SP500[2:10011])-log(datad$SP500[1:10010])) 
goldd=as.ts(log(datad$gold[2:10011])-log(datad$gold[1:10010])) 
nysed=as.ts(log(datad$NYSE[2:10011])-log(datad$NYSE[1:10010])) 


```

Again, we consider two major approaches, starting with the time series approach considering all variables then considering only the significant predictors to the model.

### Modelling Approach 1: All Predictor Time Series Model
```{r}
regmodeld <- lm(Dowd ~ Dgsd + spd + goldd + nysed)
outliersmonsigd <- which(abs(regmodeld$residuals) > 3*sqrt(var(regmodeld$residuals)))
arimamodeld <- arima(residuals(regmodeld),order = c(1,0,0))
```

```{r, fig.cap="This plot show the ACF plot for the time series model applying to all of the monthly predictors. Notice that the only component with significant autocorrelation is at 0, which is implies that this data is meeting our assumptions."}
acf(resid(arimamodeld))
```

```{r, fig.cap = "This figure shows the residuals of our first model."}
plot(ymd(datad$DATE)[2:10011], resid(arimamodeld), ylab = "Residual", xlab = "Date")
```



### Modelling Approach 2: Subsetting Predictors

Similarily,we will do a cross correlation figure. The lags shown are chosed to be within 20 days, since the goal is to analyze the data during shocks within a month, which is not covered by monthly data.

From the correlation figure, we can observe that *DOW_Close* is strongly positively correlated with DGS10, SP500, and NYSE. It might due to that all these three depend on stock market companies, while gold price depends on gold. We will consider the siginificant lags within 20 days and construct the model as in monthly data. 

```{r,fig.cap = "This figure shows cross correlation between Dow and 4 daily predictors.The x-axis is the lag, while before 0 represents past and after future. Future analysis is meaningless here since we will not do prediction. Lag that is higher than blue line means it is significant", fig.height=3}

#par(mfrow =c(2,2))
ccf(Dowd,Dgsd,ylim=range(-0.08,0.1),lag=20)
ccf(Dowd,spd,ylim=range(-0.08,0.1),lag=20)
ccf(Dowd,goldd,ylim=range(-0.08,0.1),lag=20)
ccf(Dowd,nysed,ylim=range(-0.08,0.1),lag=20)

```

```{r, figure.cap='residuals of models, showing the real effects of shocks on percent change' ,fig.height=3}
# linear model 
alldatad=ts.intersect(Dowd,
                      dgs0 =stats::lag(Dgsd,0),
                      dgs1=stats::lag(Dgsd,-1),
                      dgs3=stats::lag(Dgsd,-3),
                      dgs7=stats::lag(Dgsd,-7),
                      dgs9=stats::lag(Dgsd,-9),
                      dgs15=stats::lag(Dgsd,-15),
                      sp0=stats::lag(spd,0),
                      sp1=stats::lag(spd,-1),
                      sp4=stats::lag(spd,-4),
                      sp6=stats::lag(spd,-6),
                      sp8=stats::lag(spd,-8),
                      sp9=stats::lag(spd,-9),
                      sp12=stats::lag(spd,-12),
                      sp15=stats::lag(spd,-15),
                      sp16=stats::lag(spd,-16),
                      sp18=stats::lag(spd,-18),
                     gold=stats::lag(goldd,-2),  
                     gold9=stats::lag(goldd,-9), 
                     n=stats::lag(nysed,0),
                     n1=stats::lag(nysed,-1),
                     n4=stats::lag(nysed,-4),
                     n6=stats::lag(nysed,-6),
                     n12=stats::lag(nysed,-12),
                     n15=stats::lag(nysed,-15),
                     n16=stats::lag(nysed,-16),
                     n18=stats::lag(nysed,-18))

modelccfd=lm(Dowd~dgs0+dgs1+dgs3+dgs7+dgs9+dgs15+sp0+sp1+sp4+sp6+sp8+sp9+sp12+
               sp15+sp16+sp18+gold+gold9+n+n1+n4+n6+n12+n15+n16+n18,data=alldatad)
summary(modelccfd)

plot(ymd(datad$DATE[19:10010]),resid(modelccfd),xlab="Date",ylab="percent change of Dow_Close")
points(ymd(datad$DATE)[1713:1734],resid(modelccfd)[1694:1715],col="red")
points(ymd(datad$DATE)[2429:2451],resid(modelccfd)[2410:2432],col="red")
points(ymd(datad$DATE)[4452:4472],resid(modelccfd)[4433:4453],col="red")
points(ymd(datad$DATE)[5231:5245],resid(modelccfd)[5212:5226],col="red")
points(ymd(datad$DATE)[7009:7031],resid(modelccfd)[6990:7012],col="red")
points(ymd(datad$DATE)[7093:7111],resid(modelccfd)[7074:7092],col="red")
points(ymd(datad$DATE)[9881:9902],resid(modelccfd)[9862:9883],col="red")



```

## Considering the Significance of the Shock Residual Magnitudes

In order to test if the magnitudes of the residuals for shock periods are larger than for non-shock periods, we used permutation testing, the methods and results for which are given in much more detail in Appendix D. The results showed that the monthly analysis shows very strong evidence that the shocks have higher magnitude residuals in both models. The daily analysis also showed evidence that the shock periods, both those identified using the methods for daily analysis and for those identified using monthly analysis, have higher magnitude residuals, though not quite as strong of evidence compared to the monthly.

# Discussion

For the study, we use monthly and daily *DOW_Close* value to be our reponse variables. By the residuals of *DOW_Close* with time, we recognize 7 shocks in total. To anaalyze the real effects of these shocks, we run a model on both monthly and daily data using the assumptions of economic structure. 

The result of monthly data shows that all shocks create 10% decrease in monthly *DOW_Close* values. Thus, we assume natrual fluctuations in a month occur in a range of 10%. Among the shocks, we recognize that almost all shocks created 10% decrease except for the Black Monday Stock Market Shock in October, 1987. The reason we assume is that this shock has direct impact on stock market. On the other hand, other shocks are stable on 10% decrease because economy and government have the ability to adjust the market. 



# Limitations and Future Study

One limitation in the study is the limitation on response variable. *DOW_Close* is the predictor on stock market, thus, the study might have bias that economy is concentrated on stock market. This is also the reason why shocks on stock moarket tend to have larger effects. Other and more comprehensive predictors should be considered in the future study. 


# Works Cited

Board of Governors of the Federal Reserve System (US), 10-Year Treasury Constant Maturity Rate [DGS10], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/DGS10, November 30, 2020.

Board of Governors of the Federal Reserve System (US), Effective Federal Funds Rate [DFF], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/DFF, September 6, 2020.

^DJA historical prices, DOW Jones Composite Average common stock. (n.d.). Retrieved September 6, 2020 from https://finance.yahoo.com/quote/%5EDJA/history?p=%5EDJA, September 6, 2020

Federal Reserve Bank of St. Louis, Spot Crude Oil Price: West Texas Intermediate (WTI) [WTISPLC], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/WTISPLC, September 6, 2020.

^GSPC historical prices, S&P 500 common stock. (n.d.). Retrieved September 6, 2020 from https://finance.yahoo.com/quote/%5EGSPC?p=^GSPC&.tsrc=fin-srch, September 6, 2020

ICE Benchmark Administration Limited (IBA), Gold Fixing Price 10:30 A.M. (London time) in London Bullion Market, based in U.S. Dollars [GOLDAMGBD228NLBM], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/GOLDAMGBD228NLBM, September 6, 2020.

^NYA historical prices, NYSE Composite (DJ) common stock. (n.d.). Retrieved September 6, 2020 from https://finance.yahoo.com/quote/%5ENYA?p=^NYA&.tsrc=fin-srch, September 6, 2020

Surveys of Consumers, University of Michigan, University of Michigan: Consumer Sentiment [UMCSENT], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/UMCSENT, September 6, 2020.

U.S. Bureau of Labor Statistics, Consumer Price Index for All Urban Consumers: All Items in U.S. City Average [CPIAUCSL], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/CPIAUCSL, September 6, 2020.

U.S. Bureau of Labor Statistics, Unemployment Rate [UNRATE], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/UNRATE, September 6, 2020.

Zekra, L. (2020). COVID-19 Pandemic and Global Economic Impact. *Ovidius University Annals, Economic Sciences Series*, 0(1), 237-244. Retrieved September 22, 2020, from https://ideas.repec.org/a/ovi/oviste/vxxy2020i1p237-244.html This source breaks down the economic impacts of COVID-19. It discusses impacts from the disease itself as well as from measures taken to combat the disease. It also discusses individual industries, both those hurt and those helped by the pandemic.


Zeshan, M. (2020). Double-hit scenario of Covid-19 and global value chains. *Environment, Development and Sustainability*. doi:10.1007/s10668-020-00982-w This article considers which sectors of the economy were most hit by COVID-19 and found that all sectors had at least mild impact with some larger than others.

# Appendix A: Full Variable Description
```{r}
date <- c("DATE", "", "Date on which observations were taken; reported as the first of the month")
dow <- c("DOW", "Dollars", 
         "Gives the value of the DOW Jones Composite as of closing on the first \nof the month")
unemployment <- c("unemployment", "", 
                  "US unemployment rate (unemployed/total people)")
oil <- c("oil", "dollars/barrel",
         "Price of oil; based out of West Texas")
gold <- c("gold", "dollers/ounce",
          "Price of gold as of the first of the month; based on London Bullion Market")
cpi <- c("cpi", "relative to 1982-1984" , 
         "Consumer price index; amount that 1 dollar is worth in 1982-1984 dollars")
ffr <- c("ffr", "", 
         "Federal Funds Rate set by the Federal Reserve")
cc <- c("consumer_confidence", "",
"Consumer confidence using the University of Michigan Consumer Sentiment 
        index with 1966 as the base year")
varExplanations <- matrix(c(date, dow, unemployment, oil, gold, cpi, ffr, cc), 
                          ncol = 3, byrow = TRUE)
varExplanations <- as.data.frame(varExplanations)
colnames(varExplanations) <- c("Variable Name", "Unit", "Description")
knitr::kable(varExplanations, 
             caption = "Full description of all monthly variables",
             align = c('c', rep('l', 2))) %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "condensed"),
                          font_size = 9, latex_options = "HOLD_position")%>%
  kableExtra::column_spec(3, width = "30em")
```

```{r}
date <- c("DATE", "", "Date on which observations were taken; reported as the first of the month")
dow <- c("DOW", "dollars", "Value of the DOW Jones Composite as of closing on the particular day")
nyse <- c("nyse", "dollars", 
          "Value for the New York Stock Exchange on the particular day as of closing")
bond <- c("bond10", "dollars", "10 year US bond prices")
sp <- c("sp", "dollars", "Value of the S&P 500 Index")
gold <- c("gold", "dollers/ounce", "Price of gold; based on London Bullion Market")
varExplanations <- matrix(c(date, dow, nyse, bond, sp, gold), ncol = 3, byrow = TRUE)
varExplanations <- as.data.frame(varExplanations)
colnames(varExplanations) <- c("Variable Name", "Unit", "Description")
knitr::kable(varExplanations, 
             caption = "Full description of all daily variables",
             align = c('c', rep('l', 2))) %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "condensed"),
                          font_size = 9, latex_options = "HOLD_position")
```

Note that the CPI variable can be transformed to monthly inflation by considering the percent change of the variable. The remaining variables are relatively common metrics of economic performance. Gold is generally considered a more safe investment in uncertain times, meaning that gold prices tend to rise as the economy worsens according to theory. Similarly, unemployment is expected to rise when the economy does poorly. In the opposite direction, we expect the federal funds rate to lower as the economy does poorly as the Federal Reserve will attempt to stimulate the economy, and we expect consumer confidence to likewise lower as the economy as a whole does worse. Other variables, like S&P 500 and the NYSE are expected to follow rather closely to the DOW as these are also more cumulative metrics of economic performance. Some variables, like inflation and oil prices, don't have as consistent of a relationship, though higher oil prices tend to be worse for the economy, and inflation is generally targeted around 2% annually. Lastly, 10 year bond prices act as a metric for beliefs about future interest rates, which will be related to interest and inflation rates.

# Appendix B: Additional Plots and Exploratory Analysis

## Monthly Data
```{r appb1, fig.cap="These plots show the monthly exploratory variables over time with the DOW also shown for reference. The blue vertical lines show the shock periods identified by the analysis. Notice that in the explanatory variables, the peaks and periods of larger change appear correlated with the movements of the DOW. Likewise, the shocks often appear in times where the explanatory variables are experiencing change."}
par(mfrow =c(2,3))
for(i in c(2:6)){
  plot(ymd(data$DATE), data[,i], 
       ylab = colnames(data)[i], xlab = 'Date', main = colnames(data)[i])
  abline(v = ymd(data$DATE[outliersmonsig]), col = "blue") 
}
```

```{r appb2, fig.cap = "These plots show the DOW plotted by the explanatory variables over time. For the unemployment rate, we can see a negative relationship with the DOW, even as the magnitude of the DOW increases overtime. The remaining variables seem to have a stronger relationship with time, though in cases like the federal funds rate this can lead to apparent relationships with the DOW, in the case of the federal funds rate appearing to have a negative relationship and in the case of gold prices appearing as a positive relationship."}
dataLong <- pivot_longer(data, cols = 3:6, names_to = "Var", values_to = "Val")
ggplot(aes(x = Val, y = (DOW_Close), color = ymd(DATE)), data = dataLong) +
     geom_point() +
  facet_wrap(facets = vars(Var),scales="free") + 
  labs(color = "Date", y = "log DOW", x = "Explanatory Variables", 
       title = "DOW vs. Explanatory Variables over Time")
```

```{r appb3, fig.cap="These plots show the monthly log changes in the explanatory variables with the DOW as reference. The blue lines show the shock periods. Notice how some variables, such as oil price, have large changes near several of the shock periods while others, such as gold prices, do not have any corresponding large residual points to correspond with the shocks. Also keep in mind that log differences act similarly to percentage change, and as such the seemingly large changes in the federal funds rate in the 2010s comes from the very small magnitude of the federal funds rate in that period combined with the fact that it is generally changed in increments of .25. "}
dataDiffs <- data
for(col in 2:ncol(data)){
  for(row in 2:nrow(data)){
    dataDiffs[row, col] <- log(data[row, col]) - log(data[row-1, col])
  }
}
dataDiffs <- dataDiffs[2:nrow(dataDiffs),]
par(mfrow =c(2,3))
for(i in c(2:6)){
  plot(ymd(dataDiffs$DATE), dataDiffs[,i], ylab = colnames(dataDiffs)[i], 
       xlab = 'Date', main = colnames(data)[i])
  abline(v = ymd(dataDiffs$DATE[outliersmonsig]), col = "blue") 
}
```

From the monthly variables, we can see several patterns emerging between the different shock periods. For instance, the Great Recession and COVID-19 related shocks tend to have similar changes among the variables with larger magnitudes of changes relative to other variables. Exceptions to this trend include unemployment, which had a more gradual increase during the Great Recession compared to COVID-19 and gold prices, which were more of a gradual increase in COVID-19 relative to other shocks like the Great Recession shocks and the start of the Gulf War.

## Daily Data

```{r appb4, fig.cap="These plots show the daily exploratory variables over time with the DOW also shown for reference. The blue vertical lines show the shock periods identified by the analysis. Notice that in the explanatory variables, the peaks and periods of larger change appear correlated with the movements of the DOW. Likewise, the shocks often appear in times where the explanatory variables are experiencing change."}
par(mfrow =c(2,3))
for(i in c(2:6)){
  plot(ymd(datad$DATE), datad[,i], ylab = colnames(datad)[i], xlab = 'Date', 
       main = colnames(datad)[i])
  abline(v = ymd(data$DATE[outliersmonsig]), col = "blue")
abline(v = ymd(data$DATE[outliersmonsig+1]), col = "blue") 
}
```




```{r appb5, fig.cap = "These plots show the DOW plotted by the daily explanatory variables over time. All of the variables seem to have a relationship with the DOW, though notably this could be equally stated as having a particular relationship with time. All but bond prices (DGS10) have a positive relationship with DOW and time, bond prices having a negative relationship."}
datad$DGS10 <- as.numeric(datad$DGS10)
datad$gold <- as.numeric(datad$gold)
dataLongd <- pivot_longer(datad, cols = 3:6, names_to = "Var", values_to = "Val")
ggplot(aes(x = Val, y = (DOW), color = ymd(DATE)), data = dataLongd) +
     geom_point() +
  facet_wrap(facets = vars(Var),scales="free") + 
  labs(color = "Date", y = "DOW", x = "Explanatory Variables", 
       title = "DOW vs. Explanatory Variables over Time")
```



```{r appb6, fig.cap="These plots show the daily log changes in the explanatory variables with the DOW as reference. Notice that each of the variables has increased variability around most if not all of the shock periods."}
dataDiffsd <- datad
for(col in 2:ncol(datad)){
  for(row in 2:nrow(datad)){
    dataDiffsd[row, col] <- log(datad[row, col]) - log(datad[row-1, col])
  }
}
dataDiffsd <- dataDiffsd[2:nrow(dataDiffsd),]
par(mfrow =c(2,3))
for(i in c(2:6)){
  plot(ymd(dataDiffsd$DATE), dataDiffsd[,i], ylab = colnames(dataDiffsd)[i], 
       xlab = 'Date', main = colnames(datad)[i])
  abline(v = ymd(dataDiffs$DATE[outliersmonsig]), col = "blue") 
}
```

The similarity between the COVID-19 shock and the Great Recession shocks is even more clear in the daily data. These two periods nearly across the board have the most volatility of any of the periods, with the exception of Black Monday in some instances and COVID-19 gold prices, which were less volatile. To some extent, the shocks are all recognizable in the predictor variables, though some cases are more subtle than others, for example with the bond prices where the Gulf War has only a very small impact.

# Appendix C: Assumption checks for modelling economic structure


# Appendix D: Full Permutation Test Descriptions

For each of our modelling approaches, we conducted a permutation test to test if the residuals in the shock periods were significantly greater in magnitude than the other residuals. These tests were conducted by comparing the sample arithmetic mean of the shock period residual magnitudes with a distribution of the sample arithmetic means of random samples from the original residuals of the same size as the number of shock periods. For each case, we considered 10000 such random samples with the proportion of samples having a higher magnitude residual constituting the p value. 

To state this procedure more formally, we used permutation testing to test the hypothesis that the magnitude of the residuals for shock periods is greater than the residual magnitudes of the other points against the alternative that they are the same.

```{r permtest1, fig.cap="This figure shows the sampling distribution generated for monthly model 1 through permutation testing. The red line shows the observed value. The value for all 10000 samples is less than the observed value, providing strong support that the shock residuals have greater magnitude than the other residuals.", fig.height=3}
# Permutation testing model 1 monthly
T0 <- abs(mean(resid(arimamodel)[outliersmonsig]) - mean(resid(arimamodel)[-outliersmonsig]))
num_perm<-10000
nOutliers <- length(outliersmonsig)
T_perm<-numeric(num_perm)
for(i in 1:num_perm){
  temp <- sample(resid(arimamodel))
  T_perm[i] <- abs(mean(temp[1:nOutliers], na.rm = T) - 
                     mean(temp[nOutliers+1:length(temp)], na.rm = T))
  }
hist(T_perm, xlim = c(0,.16), main = "Samples Generated by Permutation Testing")
abline(v=T0, col = "red")
p1 <- mean(abs(T_perm)>=abs(T0))
```


```{r permtest2, fig.cap="This figure shows the sampling distribution generated for monthly model 2 through permutation testing. The red line shows the observed value. The value for all but 3 of the 10000 samples is less than the observed value, i.e. the p value is .0003, providing strong support that the shock residuals have greater magnitude than the other residuals.", fig.height=3}
# Permutation testing model 2 monthly
T0 <- abs(mean(modelccf$residuals[outliersmonsig-4]) - 
            mean(modelccf$residuals[-(outliersmonsig-4)]))
num_perm<-10000
nOutliers <- length(outliersmonsig-4)
T_perm<-numeric(num_perm)
for(i in 1:num_perm){
  temp <- sample(modelccf$residuals)
  T_perm[i] <- abs(mean(temp[1:nOutliers], na.rm = T) - 
                     mean(temp[nOutliers+1:length(temp)], na.rm = T))
  }
hist(T_perm, xlim = c(0,.16), main = "Samples Generated by Permutation Testing")
abline(v=T0, col = "red")
p2 <- mean(abs(T_perm)>=abs(T0))
```

```{r permtest3, fig.cap="This figure shows the sampling distribution generated for daily model 1 through permutation testing. The red line shows the observed value. The value for --- of the 10000 samples is less than the observed value, i.e. a p value of ---, providing support that the shock residuals have greater magnitude than the other residuals.", fig.height=3}
# Permutation testing model 1 daily
T0 <- abs(mean(resid(arimamodeld)[outliersmonsigd-19]) - 
            mean(resid(arimamodel)[-outliersmonsigd-19]))
num_perm<-10000
nOutliers <- length(outliersmonsigd-19)
T_perm<-numeric(num_perm)
for(i in 1:num_perm){
  temp <- sample(resid(arimamodeld))
  T_perm[i] <- abs(mean(temp[1:nOutliers], na.rm = T) - 
                     mean(temp[nOutliers+1:length(temp)], na.rm = T))
  }
hist(T_perm, main = "Samples Generated by Permutation Testing")
abline(v=T0, col = "red")
p3 <- mean(abs(T_perm)>=abs(T0))
```



```{r permtest32, fig.cap="This figure shows the sampling distribution generated for daily model 1 through permutation testing. The red line shows the observed value. The value for --- of the 10000 samples is less than the observed value, i.e. a p value of ---, providing support that the shock residuals, using the same shocks as in the monthly data, have greater magnitude than the other residuals.", fig.height=3}
# Permutation testing model 1 daily
outd <- c(1694:1715, 2410:2432, 4433:4453, 5212:5226, 6990:7012, 7074:7092, 9862:9883)
T0 <- abs(mean(resid(arimamodeld)[outd]) - mean(resid(arimamodeld)[-outd]))
num_perm<-10000
nOutliers <- length(outd)
T_perm<-numeric(num_perm)
for(i in 1:num_perm){
  temp <- sample(resid(arimamodeld))
  T_perm[i] <- abs(mean(temp[1:nOutliers], na.rm = T) - 
                     mean(temp[nOutliers+1:length(temp)], na.rm = T))
  }
hist(T_perm, main = "Samples Generated by Permutation Testing")
abline(v=T0, col = "red")
p4 <- mean(abs(T_perm)>=abs(T0))
```


```{r permtest4, fig.cap="This figure shows the sampling distribution generated for daily model 2 through permutation testing. The red line shows the observed value. The value for --- of the 10000 samples is less than the observed value, i.e. a p value of ---, providing support that the shock residuals have greater magnitude than the other residuals.", fig.height=3}
# Permutation testing model 1 monthly
T0 <- abs(mean(modelccfd$residuals[outliersmonsigd-19]) - 
            mean(modelccfd$residuals[-(outliersmonsigd-19)]))
num_perm<-10000
nOutliers <- length(outliersmonsigd-19)
T_perm<-numeric(num_perm)
for(i in 1:num_perm){
  temp <- sample(modelccfd$residuals)
  T_perm[i] <- abs(mean(temp[1:nOutliers], na.rm = T) - 
                     mean(temp[nOutliers+1:length(temp)], na.rm = T))
  }
hist(T_perm, main = "Samples Generated by Permutation Testing")
abline(v=T0, col = "red")
p5 <- mean(abs(T_perm)>=abs(T0))
```


```{r permtest5, fig.cap="This figure shows the sampling distribution generated for daily model 2 through permutation testing. The red line shows the observed value. The value for --- of the 10000 samples is less than the observed value, i.e. a p value of ---, providing support that the shock residuals, using the same shocks as in the monthly data, have greater magnitude than the other residuals.", fig.height=3}
# Permutation testing model 1 monthly
outd <- c(1694:1715, 2410:2432, 4433:4453, 5212:5226, 6990:7012, 7074:7092, 9862:9883)
T0 <- abs(mean(modelccfd$residuals[outd]) - mean(modelccfd$residuals[-(outd)]))
num_perm<-10000
nOutliers <- length(outd)
T_perm<-numeric(num_perm)
for(i in 1:num_perm){
  temp <- sample(modelccfd$residuals)
  T_perm[i] <- abs(mean(temp[1:nOutliers], na.rm = T) - 
                     mean(temp[nOutliers+1:length(temp)], na.rm = T))
  }
hist(T_perm, main = "Samples Generated by Permutation Testing")
abline(v=T0, col = "red")
p6 <- mean(abs(T_perm)>=abs(T0))
```

The below table summarizes the results from the above permutation tests. Lower p values indicate stronger support for the hypothesis that the shock periods have higher magnitude residuals than the other periods.

```{r}
ps <- c(p1, p2, p3, p4, p5, p6)
df <- data.frame(`p Value` = ps)
row.names(df) <- c("Monthly Model 1", 
                   "Monthly Model 2", 
                   "Daily Model 1 Using Daily Calculated Shocks", 
                   "Daily Model 1 Using Monthly Calculated Shocks", 
                   "Daily Model 2 Using Daily Calculated Shocks", 
                   "Daily Model 2 Using Monthly Calculated Shocks")
knitr::kable(df, 
             caption = "P values for each permutation test",
             align = c('c', rep('l', 2))) %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "condensed"),
                          font_size = 9, latex_options = "HOLD_position")
```


# R Script
```{r codeAppendix, ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE}
# Reprinted code chunks used previously for analysis
```
